{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNk89GfmFvEJyzB3KkTgDVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aartizikre150/DAB-303/blob/main/DAB303_Group_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn\n",
        "!pip install category_encoders\n",
        "!pip install matplotlib seaborn"
      ],
      "metadata": {
        "id": "xqIkzWerpkO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si718Cdnjq-w"
      },
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/content/drive/MyDrive/DAB303/Project1/E-Commerce Churn Data.csv')"
      ],
      "metadata": {
        "id": "FbrqCIZSj4Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimension of the data\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "Jl-Tey58kIch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the information of the data\n",
        "data.info()"
      ],
      "metadata": {
        "id": "I_SliHPqkj22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 records of the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kpcjgkLTk0Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data description\n",
        "print(data.describe(include='all'))"
      ],
      "metadata": {
        "id": "_8evmtC-k58Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print count of NA value in columns.\n",
        "null_counts = data.isnull().sum()\n",
        "print(null_counts)"
      ],
      "metadata": {
        "id": "tPS4bH_Ck-zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"Mobile Phone\" with \"Phone\" in the 'PreferredLoginDevice' column\n",
        "data['PreferredLoginDevice'] = data['PreferredLoginDevice'].replace('Mobile Phone', 'Phone')\n",
        "\n",
        "# Replace \"Cash on Delivery\" with \"COD\" in the 'PreferredPaymentMode' column\n",
        "data['PreferredPaymentMode'] = data['PreferredPaymentMode'].replace('Cash on Delivery', 'COD')\n",
        "data['PreferredPaymentMode'] = data['PreferredPaymentMode'].replace('Credit Card', 'CC')\n",
        "\n",
        "# Replace \"Mobile Phone\" with \"Phone\" in the 'PreferedOrderCat' column\n",
        "data['PreferedOrderCat'] = data['PreferedOrderCat'].replace('Mobile Phone', 'Phone')"
      ],
      "metadata": {
        "id": "tzbydTR7clgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical to numerica conversion\n",
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.OrdinalEncoder(cols=['PreferredLoginDevice', 'PreferredPaymentMode', 'Gender', 'PreferedOrderCat', 'MaritalStatus'])\n",
        "\n",
        "# Fit and transform the encoder on the DataFrame\n",
        "data_encoded = encoder.fit_transform(data)\n",
        "data_encoded.head()"
      ],
      "metadata": {
        "id": "51wZdPmTvRIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty DataFrame to store missing values and corresponding rows\n",
        "missing_data_encoded = pd.DataFrame(columns=data_encoded.columns)\n",
        "\n",
        "# Specify the columns you want to check for missing values\n",
        "columns_to_check = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder']\n",
        "\n",
        "# Find rows with missing values in the specified columns\n",
        "missing_rows = data_encoded[data_encoded[columns_to_check].isna().any(axis=1)]\n",
        "\n",
        "# Append the missing rows to the missing_data_encoded DataFrame\n",
        "missing_data_encoded = pd.concat([missing_data_encoded, missing_rows], ignore_index=True)\n",
        "\n",
        "# Remove rows with missing values in the specified columns from data_encoded\n",
        "data_encoded.dropna(subset=columns_to_check, inplace=True)\n",
        "\n",
        "# Reset the index for both dataframes\n",
        "data_encoded.reset_index(drop=True, inplace=True)\n",
        "missing_data_encoded.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Make the data types of missing_data_encoded match data_encoded\n",
        "missing_data_encoded = missing_data_encoded.astype(data_encoded.dtypes)"
      ],
      "metadata": {
        "id": "oAUmRdPJPFyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Loop through each target variable\n",
        "target_variables = ['Tenure', 'WarehouseToHome', 'HourSpendOnApp', 'OrderAmountHikeFromlastYear', 'CouponUsed', 'OrderCount', 'DaySinceLastOrder']\n",
        "for target_variable in target_variables:\n",
        "    # Step 1: Split data_encoded into features and target\n",
        "    X = data_encoded.drop(columns=[target_variable])  # Features\n",
        "    y = data_encoded[target_variable]  # Target variable\n",
        "\n",
        "    # Step 2: Create an imputer to handle missing values for both features and target\n",
        "    feature_imputer = SimpleImputer(strategy='mean')\n",
        "    target_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "    # Fit the imputers on your feature matrix X and target variable y\n",
        "    feature_imputer.fit(X)\n",
        "    target_imputer.fit(y.values.reshape(-1, 1))  # Reshape y to be a 2D array\n",
        "\n",
        "    # Transform X to replace missing values with the mean\n",
        "    X_imputed = feature_imputer.transform(X)\n",
        "\n",
        "    # Transform y to replace missing values with the mean\n",
        "    y_imputed = target_imputer.transform(y.values.reshape(-1, 1))\n",
        "\n",
        "    # Flatten y_imputed back to a 1D array\n",
        "    y_imputed = y_imputed.flatten()\n",
        "\n",
        "    # Step 3: Create and train a RandomForestRegressor model\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of trees as needed\n",
        "    model.fit(X_imputed, y_imputed)\n",
        "\n",
        "    # Step 4: Predict missing values in missing_data_encoded\n",
        "    missing_data_features = missing_data_encoded.drop(columns=[target_variable])  # Features for missing data\n",
        "    missing_data_encoded[target_variable] = model.predict(feature_imputer.transform(missing_data_features))\n",
        "\n",
        "# Drop rows with NaN values\n",
        "data_encoded.dropna(inplace=True)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "data_encoded = pd.concat([data_encoded, missing_data_encoded]).sort_values(by='CustomerID').reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "Tb_40yRE8g8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_encoded"
      ],
      "metadata": {
        "id": "gPlEVpEc_M4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with your data\n",
        "# If your DataFrame contains non-numeric columns, you may need to encode or exclude them for the heatmap\n",
        "# For this example, we'll include all columns\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data_encoded.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of DataFrame Columns')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fVKEIHaETfQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The distribution of churn (Yes/No). This will help you understand the churn rate in your dataset.\n",
        "\n",
        "# Count the number of customers in each churn category\n",
        "churn_counts = data_encoded['Churn'].value_counts()\n",
        "\n",
        "# Create a bar chart\n",
        "plt.bar(churn_counts.index, churn_counts.values)\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(churn_counts.index, ['No', 'Yes'])\n",
        "plt.title('Churn Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RKa_9niV_Z0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The distribution of customer tenure. This can help you identify common tenure ranges.\n",
        "\n",
        "# Create a histogram\n",
        "plt.hist(data_encoded['Tenure'], bins=10, edgecolor='k')\n",
        "plt.xlabel('Tenure')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Customer Tenure Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "owEWwGt8A9HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the relationship between the percentage increase in order amount from the last year and the total number of coupons used in the last month\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_encoded['OrderCount'], data_encoded['CashbackAmount'], c='blue', alpha=0.7)\n",
        "plt.xlabel('Order Count (Last Month)')\n",
        "plt.ylabel('Cashback Amount (Average)')\n",
        "plt.title('Scatter Plot: Order Count vs. Cashback Amount')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eS-nvcdjN-Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationship between the total number of coupons used in the last month and the total number of orders placed in the last month for your customers.\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data_encoded['CouponUsed'], data_encoded['OrderCount'], c='green', alpha=0.7)\n",
        "plt.xlabel('Coupon Used (Last Month)')\n",
        "plt.ylabel('Order Count (Last Month)')\n",
        "plt.title('Scatter Plot: Coupon Used vs. Order Count')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AWb-db1FT8Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot to visualize the distribution of complaints among churned and non-churned customers\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data_encoded, x='Complain', hue='Churn')\n",
        "plt.title('Complaints vs. Churn')\n",
        "plt.xlabel('Complaint')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Churn', labels=['No Churn', 'Churn'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dnlfDuafU9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already imported your data and encoded it as 'data_encoded'\n",
        "\n",
        "# Group the data and create a stacked bar plot\n",
        "grouped_data = data_encoded.groupby(['PreferredLoginDevice', 'Churn']).size().unstack().plot(kind='bar', stacked=True)\n",
        "\n",
        "# Set the plot title, x-label, and y-label\n",
        "plt.title('Churn by PreferredLoginDevice ')\n",
        "plt.xlabel('PreferredLoginDevice')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Customize the x-axis labels\n",
        "plt.xticks([0, 1], ['Phone', 'Computer'])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8xcsBYYxi7wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = data_encoded.drop(columns=[\"Churn\"])\n",
        "y = data_encoded['Churn']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "y_train_pred = dt_classifier.predict(X_train)\n",
        "y_test_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the training data\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate the accuracy of the model on the test data\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Decision Tree Training Accuracy:\", train_accuracy)\n",
        "print(\"Decision Tree Testing Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "XqK_U2JBluWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = data_encoded.drop(columns=[\"Churn\"])\n",
        "y = data_encoded['Churn']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression_model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "\n",
        "# Calculate and print the training score (mean accuracy)\n",
        "training_score = logistic_regression_model.score(X_train, y_train)\n",
        "print(\"Logistic Regression Training Score:\", training_score)\n",
        "\n",
        "# Calculate and print the testing score (mean accuracy)\n",
        "testing_score = logistic_regression_model.score(X_test, y_test)\n",
        "print(\"Logistic Regression Testing Score:\", testing_score)\n"
      ],
      "metadata": {
        "id": "iEgOZ6IS57Uc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}